{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTD Anomaly Detector with DistillBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install all the required libraries\n",
    "#####\n",
    "All the files are under the modules folder: \n",
    "- distilbert_st \n",
    "- logs_handler\n",
    "- rtd_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.distillbert_st import *\n",
    "from modules.logs_handler import *\n",
    "from modules.metric_training_callback import *\n",
    "from modules.prophet_model import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LOGS_Handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_logs_file_path = 'logs/NormalLogs.txt'\n",
    "anomalous_logs_file_path = 'logs\\AnomoulousLogs.txt'\n",
    "logs_handler = LogsHandler(normal_logs_file_path,anomalous_logs_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the operation to prepare logs for the classification \n",
    "- balance normal and anomalous logs \n",
    "- concate normal and anomalous logs previously balanced \n",
    "- reduce feature to evaluate from the logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate and balance logs \n",
    "logs_handler._concate_logs()\n",
    "\n",
    "# reduce feature of the logs \n",
    "logs_handler._reduce_feature_logs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DistillBERT_ST Class \n",
    "This class contains all the methods to process the machine learning algorithm with the NLP DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilber_st = DistillBERT_ST(logs_handler)\n",
    "predictor = Prophet_ST()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define tokenizer e Model: \n",
    " - tokenizer: A tokenizer is a tool that converts raw text into a format that a machine learning model can understand. Specifically, it breaks down the text into smaller units called tokens. These tokens are then converted into numerical representations that can be fed into a model. \n",
    " - model: In the context of NLP, a model is a machine learning algorithm that has been trained to understand and generate human language. The model takes the tokenized input and processes it to perform tasks such as classification, translation, or text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilber_st._define_model()\n",
    "distilber_st._control_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Split logs in train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = distilber_st._split_dataset(test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Tokenize train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings, test_encodings = distilber_st._tokenization(train_texts, test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset = distilber_st._create_train_and_test_dataset_rtd(train_encodings, test_encodings,train_labels,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments of trainer variable\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./training_results',        # Directory to save the model and checkpoints\n",
    "    num_train_epochs=10,                    # Number of training epochs\n",
    "    per_device_train_batch_size=64,         # Batch size for training\n",
    "    per_device_eval_batch_size=64,          # Batch size for evaluation\n",
    "    warmup_steps=500,                       # Number of warmup steps\n",
    "    weight_decay=0.01,                      # Weight decay for regularization\n",
    "    logging_dir='./training_results/logs',  # Directory to save logs\n",
    "    logging_steps=20                        # Number of steps between logging events\n",
    ")\n",
    "trainer = distilber_st._define_trainer(training_args=training_args, train_dataset=train_dataset, test_dataset=test_dataset )\n",
    "\n",
    "# Callback for the training metrics \n",
    "metrics_callback = MetricsCallback(trainer)\n",
    "trainer.add_callback(metrics_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Evaluate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Save model and tokenizer trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer in a specific folder \n",
    "distilber_st._save_model('./training_model')\n",
    "\n",
    "# Save metrics: F1Score, accuracy, precision and recall in a folder as png images \n",
    "metrics_callback.plot_and_save_metrics('./training_results/metrics12')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction of new logs from file txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load Model and tokenizer previously saved to avoid the retrain of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilber_st._define_model(load_path='training_model')\n",
    "distilber_st._control_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Load logs_to_predict file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilber_st._define_logs_to_predict(normal_logs_file_path='logs\\Logs_To_Predict.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Predict the logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilber_st._predict_logs(output_folder_path='results_prediction_logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict_future_peaks(logs_file_path='logs/Logs_To_Predict.txt', output_folder_path='results_prediction_logs', periods=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
